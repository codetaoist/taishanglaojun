# ServiceMonitor for Prometheus Operator
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: advanced-ai-service-monitor
  namespace: taishanglaojun
  labels:
    app: advanced-ai-service
    component: monitoring
spec:
  selector:
    matchLabels:
      app: advanced-ai-service
      monitoring: "true"
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
    honorLabels: true
    metricRelabelings:
    - sourceLabels: [__name__]
      regex: 'go_.*'
      action: drop
    - sourceLabels: [__name__]
      regex: 'promhttp_.*'
      action: drop

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nginx-monitor
  namespace: taishanglaojun
  labels:
    app: nginx
    component: monitoring
spec:
  selector:
    matchLabels:
      app: nginx
      monitoring: "true"
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: postgres-monitor
  namespace: taishanglaojun
  labels:
    app: postgres
    component: monitoring
spec:
  selector:
    matchLabels:
      app: postgres
      monitoring: "true"
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: redis-monitor
  namespace: taishanglaojun
  labels:
    app: redis
    component: monitoring
spec:
  selector:
    matchLabels:
      app: redis
      monitoring: "true"
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s

---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: advanced-ai-alerts
  namespace: taishanglaojun
  labels:
    app: advanced-ai-service
    component: alerts
spec:
  groups:
  - name: advanced-ai-service.rules
    interval: 30s
    rules:
    # 服务可用性告警
    - alert: ServiceDown
      expr: up{job="advanced-ai-service"} == 0
      for: 1m
      labels:
        severity: critical
        service: advanced-ai-service
      annotations:
        summary: "Advanced AI Service is down"
        description: "Advanced AI Service has been down for more than 1 minute"
    
    # 高CPU使用率告警
    - alert: HighCPUUsage
      expr: rate(container_cpu_usage_seconds_total{container="advanced-ai-service"}[5m]) * 100 > 80
      for: 5m
      labels:
        severity: warning
        service: advanced-ai-service
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% for more than 5 minutes"
    
    # 高内存使用率告警
    - alert: HighMemoryUsage
      expr: container_memory_usage_bytes{container="advanced-ai-service"} / container_spec_memory_limit_bytes{container="advanced-ai-service"} * 100 > 85
      for: 5m
      labels:
        severity: warning
        service: advanced-ai-service
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 85% for more than 5 minutes"
    
    # 高错误率告警
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100 > 5
      for: 2m
      labels:
        severity: critical
        service: advanced-ai-service
      annotations:
        summary: "High error rate detected"
        description: "Error rate is above 5% for more than 2 minutes"
    
    # 响应时间过长告警
    - alert: HighResponseTime
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
      for: 3m
      labels:
        severity: warning
        service: advanced-ai-service
      annotations:
        summary: "High response time detected"
        description: "95th percentile response time is above 1 second for more than 3 minutes"
    
    # 数据库连接池告警
    - alert: DatabaseConnectionPoolHigh
      expr: db_connections_active / db_connections_max * 100 > 80
      for: 2m
      labels:
        severity: warning
        service: database
      annotations:
        summary: "Database connection pool usage high"
        description: "Database connection pool usage is above 80%"
    
    # Redis连接数告警
    - alert: RedisConnectionsHigh
      expr: redis_connected_clients > 100
      for: 5m
      labels:
        severity: warning
        service: redis
      annotations:
        summary: "Redis connections high"
        description: "Redis has more than 100 connected clients"
    
    # 磁盘空间告警
    - alert: DiskSpaceHigh
      expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100 > 85
      for: 5m
      labels:
        severity: warning
        service: system
      annotations:
        summary: "Disk space usage high"
        description: "Disk space usage is above 85%"
    
    # AGI推理延迟告警
    - alert: AGIInferenceLatencyHigh
      expr: histogram_quantile(0.95, rate(agi_inference_duration_seconds_bucket[5m])) > 2
      for: 3m
      labels:
        severity: warning
        service: agi
      annotations:
        summary: "AGI inference latency high"
        description: "AGI inference 95th percentile latency is above 2 seconds"
    
    # 元学习训练失败告警
    - alert: MetaLearningTrainingFailed
      expr: increase(meta_learning_training_failures_total[10m]) > 3
      for: 1m
      labels:
        severity: critical
        service: meta-learning
      annotations:
        summary: "Meta-learning training failures"
        description: "More than 3 meta-learning training failures in the last 10 minutes"
    
    # 自我进化系统异常告警
    - alert: SelfEvolutionSystemError
      expr: increase(self_evolution_errors_total[5m]) > 1
      for: 1m
      labels:
        severity: critical
        service: self-evolution
      annotations:
        summary: "Self-evolution system error"
        description: "Self-evolution system has encountered errors"

  - name: infrastructure.rules
    interval: 30s
    rules:
    # Pod重启告警
    - alert: PodRestartingTooOften
      expr: increase(kube_pod_container_status_restarts_total[1h]) > 3
      for: 1m
      labels:
        severity: warning
        service: kubernetes
      annotations:
        summary: "Pod restarting too often"
        description: "Pod {{ $labels.pod }} has restarted more than 3 times in the last hour"
    
    # Pod内存不足告警
    - alert: PodMemoryOOMKilled
      expr: increase(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m]) > 0
      for: 1m
      labels:
        severity: critical
        service: kubernetes
      annotations:
        summary: "Pod killed due to OOM"
        description: "Pod {{ $labels.pod }} was killed due to out of memory"
    
    # 节点不可用告警
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
        service: kubernetes
      annotations:
        summary: "Node not ready"
        description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"

---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: taishanglaojun
  labels:
    app: grafana
    component: dashboards
data:
  advanced-ai-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "Advanced AI Service Dashboard",
        "tags": ["ai", "taishanglaojun"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total[5m])",
                "legendFormat": "{{ method }} {{ status }}"
              }
            ],
            "yAxes": [
              {
                "label": "Requests/sec"
              }
            ]
          },
          {
            "id": 2,
            "title": "Response Time",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
                "legendFormat": "95th percentile"
              },
              {
                "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
                "legendFormat": "50th percentile"
              }
            ],
            "yAxes": [
              {
                "label": "Seconds"
              }
            ]
          },
          {
            "id": 3,
            "title": "Error Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100",
                "legendFormat": "Error Rate %"
              }
            ],
            "yAxes": [
              {
                "label": "Percentage"
              }
            ]
          },
          {
            "id": 4,
            "title": "CPU Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(container_cpu_usage_seconds_total{container=\"advanced-ai-service\"}[5m]) * 100",
                "legendFormat": "{{ pod }}"
              }
            ],
            "yAxes": [
              {
                "label": "Percentage"
              }
            ]
          },
          {
            "id": 5,
            "title": "Memory Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "container_memory_usage_bytes{container=\"advanced-ai-service\"} / 1024 / 1024",
                "legendFormat": "{{ pod }}"
              }
            ],
            "yAxes": [
              {
                "label": "MB"
              }
            ]
          },
          {
            "id": 6,
            "title": "AGI Inference Metrics",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(agi_inference_total[5m])",
                "legendFormat": "Inference Rate"
              },
              {
                "expr": "histogram_quantile(0.95, rate(agi_inference_duration_seconds_bucket[5m]))",
                "legendFormat": "95th Percentile Latency"
              }
            ]
          },
          {
            "id": 7,
            "title": "Meta-Learning Metrics",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(meta_learning_training_total[5m])",
                "legendFormat": "Training Rate"
              },
              {
                "expr": "meta_learning_model_accuracy",
                "legendFormat": "Model Accuracy"
              }
            ]
          },
          {
            "id": 8,
            "title": "Self-Evolution Metrics",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(self_evolution_improvements_total[5m])",
                "legendFormat": "Improvement Rate"
              },
              {
                "expr": "self_evolution_performance_score",
                "legendFormat": "Performance Score"
              }
            ]
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }

---
# Alertmanager配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: taishanglaojun
  labels:
    app: alertmanager
    component: config
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@taishanglaojun.com'
      smtp_auth_username: 'alerts@taishanglaojun.com'
      smtp_auth_password_file: '/etc/alertmanager/secrets/smtp-password'
    
    route:
      group_by: ['alertname', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
      - match:
          severity: warning
        receiver: 'warning-alerts'
      - match:
          service: advanced-ai-service
        receiver: 'ai-service-alerts'
    
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://webhook-service:8080/alerts'
        send_resolved: true
    
    - name: 'critical-alerts'
      email_configs:
      - to: 'admin@taishanglaojun.com'
        subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
      slack_configs:
      - api_url_file: '/etc/alertmanager/secrets/slack-webhook-url'
        channel: '#alerts-critical'
        title: '[CRITICAL] {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
      pagerduty_configs:
      - routing_key_file: '/etc/alertmanager/secrets/pagerduty-key'
        description: '{{ .GroupLabels.alertname }}'
    
    - name: 'warning-alerts'
      email_configs:
      - to: 'team@taishanglaojun.com'
        subject: '[WARNING] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
      slack_configs:
      - api_url_file: '/etc/alertmanager/secrets/slack-webhook-url'
        channel: '#alerts-warning'
        title: '[WARNING] {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    
    - name: 'ai-service-alerts'
      webhook_configs:
      - url: 'http://ai-service-webhook:8080/alerts'
        send_resolved: true
      email_configs:
      - to: 'ai-team@taishanglaojun.com'
        subject: '[AI SERVICE] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Service: {{ .Labels.service }}
          {{ end }}

---
# Jaeger配置 (分布式追踪)
apiVersion: v1
kind: ConfigMap
metadata:
  name: jaeger-config
  namespace: taishanglaojun
  labels:
    app: jaeger
    component: config
data:
  jaeger.yml: |
    collector:
      zipkin:
        host-port: ":9411"
    
    storage:
      type: elasticsearch
      options:
        es:
          server-urls: http://elasticsearch:9200
          index-prefix: jaeger
    
    query:
      base-path: /jaeger
    
    agent:
      jaeger:
        thrift-compact-port: 6831
        thrift-binary-port: 6832
        thrift-http-port: 14268
      zipkin:
        thrift-port: 5775